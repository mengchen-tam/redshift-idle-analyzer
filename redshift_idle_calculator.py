#!/usr/bin/env python3
"""
Redshiftç©ºé—²æ—¶é—´è®¡ç®—å™¨

é€šè¿‡åˆ†æCloudWatchæŒ‡æ ‡æ¥è®¡ç®—Redshifté›†ç¾¤çš„ç©ºé—²æ—¶é—´ç™¾åˆ†æ¯”ï¼Œ
è¯„ä¼°è¿ç§»åˆ°Serverlessçš„æ½œåœ¨æˆæœ¬èŠ‚çœã€‚

åŠŸèƒ½ç‰¹æ€§:
- ğŸ“Š æ™ºèƒ½åˆ†æRedshifté›†ç¾¤ä½¿ç”¨æ¨¡å¼
- â±ï¸ ç²¾ç¡®è®¡ç®—ç©ºé—²æ—¶é—´ç™¾åˆ†æ¯”
- ğŸ’° è¯„ä¼°Serverlessè¿ç§»æˆæœ¬èŠ‚çœ
- ğŸŒ æ”¯æŒæ‰€æœ‰AWSåŒºåŸŸï¼ˆåŒ…æ‹¬ä¸­å›½åŒºï¼‰
- ğŸ” å†…ç½®æ•°æ®è´¨é‡æ£€æŸ¥å’Œæƒé™éªŒè¯
- ğŸ§ª å®Œæ•´çš„æµ‹è¯•å¥—ä»¶

ä½¿ç”¨æ–¹æ³•:
    # åŸºæœ¬åˆ†æ
    python redshift_idle_calculator.py --cluster-id my-cluster --region us-east-1
    
    # æŒ‡å®šåˆ†æå‘¨æœŸ
    python redshift_idle_calculator.py --cluster-id my-cluster --region us-east-1 --days 14
    
    # è¿è¡Œæµ‹è¯•
    python redshift_idle_calculator.py --test

ä½œè€…: Redshift Cost Optimizer
ç‰ˆæœ¬: 1.0.0
è®¸å¯: MIT License
"""

import argparse
import sys
import time
import json
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Any, Optional
from collections import namedtuple

import boto3
from botocore.exceptions import ClientError, NoCredentialsError

# ç‰ˆæœ¬ä¿¡æ¯
__version__ = "1.0.0"
__author__ = "Redshift Cost Optimizer"

# æ•°æ®ç»“æ„å®šä¹‰
MetricPoint = namedtuple('MetricPoint', ['timestamp', 'value'])

def get_rpu_price_dynamic(region: str) -> Dict[str, Any]:
    """
    åŠ¨æ€è·å–RPUä»·æ ¼ï¼Œä¼˜å…ˆä½¿ç”¨AWS Pricing APIï¼Œå¤±è´¥æ—¶ä½¿ç”¨å¤‡ç”¨ä»·æ ¼
    
    Args:
        region: AWSåŒºåŸŸ
        
    Returns:
        ä»·æ ¼ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«price, currency, sourceç­‰å­—æ®µ
    """
    def get_pricing_api_region(region: str) -> str:
        """ç¡®å®šPricing APIåŒºåŸŸ"""
        if region.startswith('cn-'):
            return 'cn-northwest-1'  # ä¸­å›½åŒºåŸŸçš„Pricing API
        else:
            return 'us-east-1'  # GlobalåŒºåŸŸçš„Pricing API
    
    def get_location_name(region: str) -> str:
        """åŒºåŸŸä»£ç è½¬æ¢ä¸ºä½ç½®åç§°"""
        region_mapping = {
            'us-east-1': 'US East (N. Virginia)',
            'us-west-2': 'US West (Oregon)',
            'eu-west-1': 'Europe (Ireland)',
            'ap-southeast-1': 'Asia Pacific (Singapore)',
            'cn-north-1': 'China (Beijing)',
            'cn-northwest-1': 'China (Ningxia)',
        }
        return region_mapping.get(region, region)
    
    def query_api_price(region: str) -> Optional[Dict]:
        """ä½¿ç”¨APIæŸ¥è¯¢ä»·æ ¼"""
        try:
            pricing_region = get_pricing_api_region(region)
            location = get_location_name(region)
            
            pricing_client = boto3.client('pricing', region_name=pricing_region)
            
            response = pricing_client.get_products(
                ServiceCode='AmazonRedshift',
                Filters=[
                    {'Type': 'TERM_MATCH', 'Field': 'location', 'Value': location},
                    {'Type': 'TERM_MATCH', 'Field': 'productFamily', 'Value': 'Serverless'}
                ],
                MaxResults=10
            )
            
            for product_str in response.get('PriceList', []):
                product = json.loads(product_str)
                attributes = product.get('product', {}).get('attributes', {})
                payment_option = attributes.get('paymentOption', '')
                
                terms = product.get('terms', {}).get('OnDemand', {})
                for term_key, term_value in terms.items():
                    price_dimensions = term_value.get('priceDimensions', {})
                    for price_key, price_value in price_dimensions.items():
                        unit = price_value.get('unit', '')
                        description = price_value.get('description', '')
                        price_per_unit = price_value.get('pricePerUnit', {})
                        
                        if unit == 'RPU-Hr' and price_per_unit:
                            currency = list(price_per_unit.keys())[0]
                            price = float(price_per_unit[currency])
                            
                            # åˆ¤æ–­æ˜¯å¦ä¸ºæŒ‰éœ€å®šä»·ï¼ˆä¸æ˜¯é¢„ç•™å®ä¾‹ï¼‰
                            is_on_demand = (
                                not payment_option or  # ç©ºçš„payment_optioné€šå¸¸æ˜¯æŒ‰éœ€
                                payment_option == 'On Demand' or
                                ('serverless usage' in description.lower() and 'reservations' not in description.lower())
                            )
                            
                            # è°ƒè¯•ä¿¡æ¯
                            # print(f"DEBUG: payment_option='{payment_option}', description='{description}', is_on_demand={is_on_demand}")
                            
                            if is_on_demand:
                                return {'price': price, 'currency': currency, 'source': 'api'}
            return None
        except Exception:
            return None
    
    def get_fallback_price(region: str) -> Dict:
        """è·å–å¤‡ç”¨ç¡¬ç¼–ç ä»·æ ¼"""
        fallback_prices = {
            'cn-north-1': {'price': 2.692, 'currency': 'CNY'},
            'cn-northwest-1': {'price': 2.093, 'currency': 'CNY'},
            'us-east-1': {'price': 0.375, 'currency': 'USD'},
            'us-west-2': {'price': 0.375, 'currency': 'USD'},
            'eu-west-1': {'price': 0.375, 'currency': 'USD'},
            'ap-southeast-1': {'price': 0.45, 'currency': 'USD'},
        }
        
        if region in fallback_prices:
            data = fallback_prices[region]
            return {'price': data['price'], 'currency': data['currency'], 'source': 'hardcoded'}
        else:
            return {'price': 0.375, 'currency': 'USD', 'source': 'default'}
    
    # é¦–å…ˆå°è¯•APIæŸ¥è¯¢
    api_result = query_api_price(region)
    if api_result:
        return api_result
    else:
        return get_fallback_price(region)

def get_instance_price_dynamic(node_type: str, region: str) -> Dict[str, Any]:
    """
    åŠ¨æ€è·å–Redshiftå®ä¾‹ä»·æ ¼ï¼Œä¼˜å…ˆä½¿ç”¨AWS Pricing APIï¼Œå¤±è´¥æ—¶ä½¿ç”¨å¤‡ç”¨ä»·æ ¼
    
    Args:
        node_type: å®ä¾‹ç±»å‹ (å¦‚ ra3.xlplus)
        region: AWSåŒºåŸŸ
        
    Returns:
        ä»·æ ¼ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«price, currency, sourceç­‰å­—æ®µ
    """
    def query_instance_api_price(node_type: str, region: str) -> Optional[Dict]:
        """ä½¿ç”¨APIæŸ¥è¯¢å®ä¾‹ä»·æ ¼"""
        try:
            pricing_region = 'cn-northwest-1' if region.startswith('cn-') else 'us-east-1'
            location = {
                'us-east-1': 'US East (N. Virginia)',
                'us-west-2': 'US West (Oregon)',
                'eu-west-1': 'Europe (Ireland)',
                'ap-southeast-1': 'Asia Pacific (Singapore)',
                'cn-north-1': 'China (Beijing)',
                'cn-northwest-1': 'China (Ningxia)',
            }.get(region, region)
            
            pricing_client = boto3.client('pricing', region_name=pricing_region)
            
            response = pricing_client.get_products(
                ServiceCode='AmazonRedshift',
                Filters=[
                    {'Type': 'TERM_MATCH', 'Field': 'location', 'Value': location},
                    {'Type': 'TERM_MATCH', 'Field': 'productFamily', 'Value': 'Compute Instance'},
                    {'Type': 'TERM_MATCH', 'Field': 'instanceType', 'Value': node_type}
                ],
                MaxResults=5
            )
            
            for product_str in response.get('PriceList', []):
                product = json.loads(product_str)
                
                # æŸ¥æ‰¾æŒ‰éœ€å®šä»·
                terms = product.get('terms', {}).get('OnDemand', {})
                for term_key, term_value in terms.items():
                    price_dimensions = term_value.get('priceDimensions', {})
                    for price_key, price_value in price_dimensions.items():
                        unit = price_value.get('unit', '')
                        price_per_unit = price_value.get('pricePerUnit', {})
                        
                        if unit == 'Hrs' and price_per_unit:
                            currency = list(price_per_unit.keys())[0]
                            price = float(price_per_unit[currency])
                            return {'price': price, 'currency': currency, 'source': 'api'}
            return None
        except Exception:
            return None
    
    def get_fallback_instance_price(node_type: str, region: str) -> Dict:
        """è·å–å¤‡ç”¨ç¡¬ç¼–ç å®ä¾‹ä»·æ ¼"""
        # ä¸­å›½åŒºåŸŸå®šä»·è¡¨ï¼ˆæ¯å°æ—¶äººæ°‘å¸ï¼‰
        cn_pricing_table = {
            'dc2.large': 2.145,
            'dc2.8xlarge': 41.60,
            'ra3.large': 3.475,
            'ra3.xlplus': 6.950,
            'ra3.4xlarge': 20.864,
            'ra3.16xlarge': 83.456,
        }
        
        # ç¾å›½åŒºåŸŸå®šä»·è¡¨ï¼ˆæ¯å°æ—¶ç¾å…ƒï¼‰
        us_pricing_table = {
            'dc2.large': 0.25,
            'dc2.8xlarge': 4.80,
            'ra3.large': 0.48,
            'ra3.xlplus': 1.086,
            'ra3.4xlarge': 3.26,
            'ra3.16xlarge': 13.04,
        }
        
        if region.startswith('cn-'):
            pricing_table = cn_pricing_table
            currency = 'CNY'
        else:
            pricing_table = us_pricing_table
            currency = 'USD'
        
        if node_type in pricing_table:
            price = pricing_table[node_type]
        else:
            # æœªçŸ¥å®ä¾‹ç±»å‹ä½¿ç”¨ra3.xlplusä»·æ ¼
            price = pricing_table.get('ra3.xlplus', 6.950 if region.startswith('cn-') else 1.086)
        
        return {'price': price, 'currency': currency, 'source': 'hardcoded'}
    
    # é¦–å…ˆå°è¯•APIæŸ¥è¯¢
    api_result = query_instance_api_price(node_type, region)
    if api_result:
        return api_result
    else:
        return get_fallback_instance_price(node_type, region)

def validate_inputs(cluster_id: str, region: str, days: int) -> None:
    """
    éªŒè¯è¾“å…¥å‚æ•°
    
    Args:
        cluster_id: Redshifté›†ç¾¤æ ‡è¯†ç¬¦
        region: AWSåŒºåŸŸ
        days: åˆ†æå¤©æ•°
        
    Raises:
        ValueError: å½“è¾“å…¥å‚æ•°æ— æ•ˆæ—¶
    """
    # éªŒè¯é›†ç¾¤ID
    if not cluster_id or not cluster_id.strip():
        raise ValueError("é›†ç¾¤IDä¸èƒ½ä¸ºç©º")
    
    # é›†ç¾¤IDæ ¼å¼éªŒè¯ï¼ˆåŸºæœ¬æ£€æŸ¥ï¼‰
    cluster_id = cluster_id.strip()
    if len(cluster_id) < 1 or len(cluster_id) > 63:
        raise ValueError("é›†ç¾¤IDé•¿åº¦å¿…é¡»åœ¨1-63ä¸ªå­—ç¬¦ä¹‹é—´")
    
    # éªŒè¯åŒºåŸŸ
    if not region or not region.strip():
        raise ValueError("åŒºåŸŸä¸èƒ½ä¸ºç©º")
    
    # åŸºæœ¬çš„åŒºåŸŸæ ¼å¼éªŒè¯
    valid_region_patterns = [
        'us-east-1', 'us-east-2', 'us-west-1', 'us-west-2',
        'eu-west-1', 'eu-west-2', 'eu-west-3', 'eu-central-1',
        'ap-southeast-1', 'ap-southeast-2', 'ap-northeast-1', 'ap-northeast-2',
        'cn-north-1', 'cn-northwest-1',  # ä¸­å›½åŒºåŸŸ
        'ca-central-1', 'sa-east-1'
    ]
    
    region = region.strip()
    # ç®€å•çš„åŒºåŸŸæ ¼å¼æ£€æŸ¥ï¼ˆå…è®¸è‡ªå®šä¹‰åŒºåŸŸï¼‰
    if not (region in valid_region_patterns or 
            (len(region.split('-')) >= 3 and region.replace('-', '').replace('1', '').replace('2', '').replace('3', '').isalpha())):
        print(f"âš ï¸  è­¦å‘Š: åŒºåŸŸ '{region}' å¯èƒ½ä¸æ˜¯æœ‰æ•ˆçš„AWSåŒºåŸŸ")
    
    # éªŒè¯å¤©æ•°
    if not isinstance(days, int):
        raise ValueError("åˆ†æå¤©æ•°å¿…é¡»æ˜¯æ•´æ•°")
    
    if days <= 0:
        raise ValueError("åˆ†æå¤©æ•°å¿…é¡»å¤§äº0")
    
    if days > 30:
        raise ValueError("åˆ†æå¤©æ•°ä¸èƒ½è¶…è¿‡30å¤©ï¼ˆCloudWatchæ•°æ®ä¿ç•™é™åˆ¶ï¼‰")
    
    print(f"âœ“ è¾“å…¥éªŒè¯é€šè¿‡: é›†ç¾¤={cluster_id}, åŒºåŸŸ={region}, å¤©æ•°={days}")

def validate_aws_credentials(region: str) -> bool:
    """
    éªŒè¯AWSå‡­è¯æ˜¯å¦æœ‰æ•ˆ
    
    Args:
        region: AWSåŒºåŸŸ
        
    Returns:
        Trueå¦‚æœå‡­è¯æœ‰æ•ˆï¼ŒFalseå¦åˆ™
    """
    try:
        # å°è¯•è·å–è°ƒç”¨è€…èº«ä»½
        sts = boto3.client('sts', region_name=region)
        response = sts.get_caller_identity()
        
        account_id = response.get('Account', 'unknown')
        user_arn = response.get('Arn', 'unknown')
        
        print(f"âœ“ AWSå‡­è¯éªŒè¯é€šè¿‡")
        print(f"   è´¦æˆ·ID: {account_id}")
        print(f"   ç”¨æˆ·ARN: {user_arn}")
        
        return True
        
    except NoCredentialsError:
        print("âŒ AWSå‡­è¯æœªé…ç½®")
        print("   è¯·è¿è¡Œ 'aws configure' æˆ–è®¾ç½®ç¯å¢ƒå˜é‡:")
        print("   - AWS_ACCESS_KEY_ID")
        print("   - AWS_SECRET_ACCESS_KEY")
        print("   - AWS_SESSION_TOKEN (å¦‚æœä½¿ç”¨ä¸´æ—¶å‡­è¯)")
        return False
        
    except ClientError as e:
        error_code = e.response['Error']['Code']
        if error_code == 'InvalidUserID.NotFound':
            print("âŒ AWSå‡­è¯æ— æ•ˆæˆ–å·²è¿‡æœŸ")
        elif error_code == 'AccessDenied':
            print("âŒ AWSå‡­è¯æƒé™ä¸è¶³")
        else:
            print(f"âŒ AWSå‡­è¯éªŒè¯å¤±è´¥: {e}")
        return False
        
    except Exception as e:
        print(f"âŒ AWSå‡­è¯éªŒè¯å‡ºé”™: {e}")
        return False

def validate_cluster_access(cluster_id: str, region: str) -> bool:
    """
    éªŒè¯æ˜¯å¦èƒ½è®¿é—®æŒ‡å®šçš„Redshifté›†ç¾¤
    
    Args:
        cluster_id: é›†ç¾¤æ ‡è¯†ç¬¦
        region: AWSåŒºåŸŸ
        
    Returns:
        Trueå¦‚æœèƒ½è®¿é—®ï¼ŒFalseå¦åˆ™
    """
    try:
        redshift = boto3.client('redshift', region_name=region)
        response = redshift.describe_clusters(ClusterIdentifier=cluster_id)
        
        if not response['Clusters']:
            print(f"âŒ æœªæ‰¾åˆ°é›†ç¾¤: {cluster_id}")
            return False
            
        cluster = response['Clusters'][0]
        status = cluster.get('ClusterStatus', 'unknown')
        
        print(f"âœ“ é›†ç¾¤è®¿é—®éªŒè¯é€šè¿‡")
        print(f"   é›†ç¾¤çŠ¶æ€: {status}")
        
        if status != 'available':
            print(f"âš ï¸  è­¦å‘Š: é›†ç¾¤çŠ¶æ€ä¸º '{status}'ï¼Œå¯èƒ½å½±å“æŒ‡æ ‡æ•°æ®çš„å®Œæ•´æ€§")
        
        return True
        
    except ClientError as e:
        error_code = e.response['Error']['Code']
        if error_code == 'ClusterNotFoundFault':
            print(f"âŒ é›†ç¾¤ä¸å­˜åœ¨: {cluster_id}")
            print(f"   è¯·æ£€æŸ¥é›†ç¾¤IDæ˜¯å¦æ­£ç¡®ï¼Œä»¥åŠæ˜¯å¦åœ¨åŒºåŸŸ {region} ä¸­")
        elif error_code == 'AccessDenied':
            print(f"âŒ æ— æƒé™è®¿é—®é›†ç¾¤: {cluster_id}")
            print("   è¯·ç¡®ä¿å…·æœ‰ redshift:DescribeClusters æƒé™")
        else:
            print(f"âŒ é›†ç¾¤è®¿é—®éªŒè¯å¤±è´¥: {e}")
        return False
        
    except Exception as e:
        print(f"âŒ é›†ç¾¤è®¿é—®éªŒè¯å‡ºé”™: {e}")
        return False

def validate_cloudwatch_permissions(cluster_id: str, region: str) -> bool:
    """
    éªŒè¯CloudWatchæƒé™
    
    Args:
        cluster_id: é›†ç¾¤æ ‡è¯†ç¬¦
        region: AWSåŒºåŸŸ
        
    Returns:
        Trueå¦‚æœæƒé™è¶³å¤Ÿï¼ŒFalseå¦åˆ™
    """
    try:
        cloudwatch = boto3.client('cloudwatch', region_name=region)
        
        # å°è¯•è·å–ä¸€ä¸ªç®€å•çš„æŒ‡æ ‡æ¥æµ‹è¯•æƒé™
        end_time = datetime.now(timezone.utc)
        start_time = end_time - timedelta(hours=1)
        
        response = cloudwatch.get_metric_statistics(
            Namespace='AWS/Redshift',
            MetricName='DatabaseConnections',
            Dimensions=[{'Name': 'ClusterIdentifier', 'Value': cluster_id}],
            StartTime=start_time,
            EndTime=end_time,
            Period=300,
            Statistics=['Average']
        )
        
        print(f"âœ“ CloudWatchæƒé™éªŒè¯é€šè¿‡")
        return True
        
    except ClientError as e:
        error_code = e.response['Error']['Code']
        if error_code == 'AccessDenied':
            print(f"âŒ CloudWatchæƒé™ä¸è¶³")
            print("   è¯·ç¡®ä¿å…·æœ‰ cloudwatch:GetMetricStatistics æƒé™")
        else:
            print(f"âŒ CloudWatchæƒé™éªŒè¯å¤±è´¥: {e}")
        return False
        
    except Exception as e:
        print(f"âŒ CloudWatchæƒé™éªŒè¯å‡ºé”™: {e}")
        return False

def check_data_availability(metrics: Dict[str, List[Dict]]) -> Dict[str, Any]:
    """
    æ£€æŸ¥æ•°æ®è´¨é‡å’Œå¯ç”¨æ€§
    
    Args:
        metrics: CloudWatchæŒ‡æ ‡æ•°æ®
        
    Returns:
        æ•°æ®è´¨é‡æŠ¥å‘Š
    """
    print("ğŸ” æ£€æŸ¥æ•°æ®è´¨é‡...")
    
    total_expected_points = 0
    total_actual_points = 0
    missing_metrics = []
    sparse_metrics = []
    
    for metric_name, datapoints in metrics.items():
        actual_count = len(datapoints)
        total_actual_points += actual_count
        
        if actual_count == 0:
            missing_metrics.append(metric_name)
        elif actual_count < 10:  # å°‘äº10ä¸ªæ•°æ®ç‚¹è®¤ä¸ºæ˜¯ç¨€ç–çš„
            sparse_metrics.append(f"{metric_name}({actual_count}ä¸ªç‚¹)")
    
    # è®¡ç®—æ•°æ®å®Œæ•´æ€§
    if total_actual_points > 0:
        # ä¼°ç®—æœŸæœ›çš„æ•°æ®ç‚¹æ•°ï¼ˆåŸºäºç¬¬ä¸€ä¸ªæœ‰æ•°æ®çš„æŒ‡æ ‡ï¼‰
        for datapoints in metrics.values():
            if datapoints:
                time_span = (datapoints[-1]['Timestamp'] - datapoints[0]['Timestamp']).total_seconds()
                expected_points_per_metric = int(time_span / 60) + 1  # 60ç§’é—´éš”
                total_expected_points = expected_points_per_metric * len(metrics)
                break
    
    completeness = (total_actual_points / total_expected_points * 100) if total_expected_points > 0 else 0
    
    quality_report = {
        'total_points': total_actual_points,
        'expected_points': total_expected_points,
        'completeness_percentage': completeness,
        'missing_metrics': missing_metrics,
        'sparse_metrics': sparse_metrics,
        'is_sufficient': total_actual_points > 0 and len(missing_metrics) < len(metrics)
    }
    
    print(f"   æ•°æ®å®Œæ•´æ€§: {completeness:.1f}% ({total_actual_points}/{total_expected_points})")
    
    if missing_metrics:
        print(f"   âš ï¸  ç¼ºå¤±æŒ‡æ ‡: {', '.join(missing_metrics)}")
    
    if sparse_metrics:
        print(f"   âš ï¸  ç¨€ç–æŒ‡æ ‡: {', '.join(sparse_metrics)}")
    
    if quality_report['is_sufficient']:
        print(f"âœ“ æ•°æ®è´¨é‡æ£€æŸ¥é€šè¿‡")
    else:
        print(f"âŒ æ•°æ®è´¨é‡ä¸è¶³ï¼Œå¯èƒ½å½±å“åˆ†æå‡†ç¡®æ€§")
    
    return quality_report

def print_results(cluster_id: str, region: str, days: int, analysis_result: Dict[str, Any], 
                 cost_analysis: Dict[str, float], cluster_info: Dict[str, Any], 
                 data_quality: Dict[str, Any]) -> None:
    """
    è¾“å‡ºæ ¼å¼åŒ–çš„åˆ†æç»“æœ
    
    Args:
        cluster_id: é›†ç¾¤æ ‡è¯†ç¬¦
        region: AWSåŒºåŸŸ
        days: åˆ†æå¤©æ•°
        analysis_result: æ´»è·ƒçŠ¶æ€åˆ†æç»“æœ
        cost_analysis: æˆæœ¬åˆ†æç»“æœ
        cluster_info: é›†ç¾¤ä¿¡æ¯
        data_quality: æ•°æ®è´¨é‡æŠ¥å‘Š
    """
    print("\n" + "="*60)
    print("ğŸ¯ REDSHIFTç©ºé—²æ—¶é—´åˆ†æç»“æœ")
    print("="*60)
    
    # åŸºæœ¬ä¿¡æ¯
    print(f"\nğŸ“‹ åŸºæœ¬ä¿¡æ¯:")
    print(f"   é›†ç¾¤ID: {cluster_id}")
    print(f"   AWSåŒºåŸŸ: {region}")
    print(f"   é›†ç¾¤é…ç½®: {cluster_info['node_type']} x {cluster_info['number_of_nodes']}")
    print(f"   é›†ç¾¤çŠ¶æ€: {cluster_info['cluster_status']}")
    
    # åˆ†æå‘¨æœŸ
    if analysis_result['analysis_period']:
        start_time, end_time = analysis_result['analysis_period']
        print(f"   åˆ†æå‘¨æœŸ: {start_time.strftime('%Y-%m-%d %H:%M')} ~ {end_time.strftime('%Y-%m-%d %H:%M')} ({days}å¤©)")
    
    # æ•°æ®è´¨é‡
    print(f"\nğŸ“Š æ•°æ®è´¨é‡:")
    print(f"   æ•°æ®å®Œæ•´æ€§: {data_quality['completeness_percentage']:.1f}%")
    print(f"   æ€»æ•°æ®ç‚¹: {data_quality['total_points']}")
    if data_quality['missing_metrics']:
        print(f"   ç¼ºå¤±æŒ‡æ ‡: {', '.join(data_quality['missing_metrics'])}")
    if data_quality['sparse_metrics']:
        print(f"   ç¨€ç–æŒ‡æ ‡: {', '.join(data_quality['sparse_metrics'])}")
    
    # ä½¿ç”¨æ¨¡å¼åˆ†æ
    print(f"\nâ±ï¸  ä½¿ç”¨æ¨¡å¼åˆ†æ:")
    print(f"   ç©ºé—²æ—¶é—´ç™¾åˆ†æ¯”: {analysis_result['idle_percentage']:.1f}%")
    print(f"   æ´»è·ƒæ—¶é—´ç™¾åˆ†æ¯”: {cost_analysis['active_percentage']:.1f}%")
    print(f"   æ€»æ—¶é—´ç‚¹: {analysis_result['total_points']}")
    print(f"   æ´»è·ƒæ—¶é—´ç‚¹: {analysis_result['active_points']}")
    print(f"   ç©ºé—²æ—¶é—´ç‚¹: {analysis_result['idle_points']}")
    
    # å„æŒ‡æ ‡æ´»è·ƒç»Ÿè®¡
    print(f"\nğŸ“ˆ å„æŒ‡æ ‡æ´»è·ƒç»Ÿè®¡:")
    for metric, count in analysis_result['activity_breakdown'].items():
        percentage = (count / analysis_result['total_points'] * 100) if analysis_result['total_points'] > 0 else 0
        print(f"   {metric}: {count} æ¬¡ ({percentage:.1f}%)")
    
    # æˆæœ¬åˆ†æ
    currency = cost_analysis.get('currency_symbol', 'Â¥')
    print(f"\nğŸ’° æˆæœ¬åˆ†æ:")
    print(f"   å½“å‰æœˆåº¦æˆæœ¬: {currency}{cost_analysis['current_monthly_cost']:.2f}")
    print(f"   Serverlessæ‰€éœ€RPU: {cost_analysis.get('required_rpu', 'N/A')}")
    print(f"   Serverlessé¢„ä¼°æˆæœ¬: {currency}{cost_analysis['serverless_monthly_cost']:.2f}")
    print(f"   æ½œåœ¨æœˆåº¦èŠ‚çœ: {currency}{cost_analysis['potential_savings']:.2f}")
    print(f"   èŠ‚çœç™¾åˆ†æ¯”: {cost_analysis['savings_percentage']:.1f}%")
    print(f"   ç›ˆäºå¹³è¡¡ç‚¹: ä½¿ç”¨ç‡éœ€ä½äº {cost_analysis['break_even_usage_percentage']:.1f}%")
    
    # å»ºè®®
    print(f"\nğŸ’¡ å»ºè®®:")
    if cost_analysis['savings_percentage'] > 10:
        print(f"   âœ… å¼ºçƒˆå»ºè®®è¿ç§»åˆ°Serverless")
        print(f"      - å¯èŠ‚çœ {cost_analysis['savings_percentage']:.1f}% çš„æˆæœ¬")
        print(f"      - æ¯æœˆå¯èŠ‚çœçº¦ {currency}{cost_analysis['potential_savings']:.2f}")
    elif cost_analysis['savings_percentage'] > 0:
        print(f"   âœ… å»ºè®®è€ƒè™‘è¿ç§»åˆ°Serverless")
        print(f"      - å¯èŠ‚çœ {cost_analysis['savings_percentage']:.1f}% çš„æˆæœ¬")
        print(f"      - æ¯æœˆå¯èŠ‚çœçº¦ {currency}{cost_analysis['potential_savings']:.2f}")
    else:
        print(f"   âš ï¸  å½“å‰ä½¿ç”¨æ¨¡å¼ä¸‹ï¼Œä¿æŒç°æœ‰é…ç½®å¯èƒ½æ›´ç»æµ")
        print(f"      - Serverlessåœ¨å½“å‰ä½¿ç”¨ç‡ä¸‹ä¼šå¢åŠ  {abs(cost_analysis['savings_percentage']):.1f}% çš„æˆæœ¬")
        print(f"      - å¦‚æœä½¿ç”¨ç‡èƒ½é™ä½åˆ° {cost_analysis['break_even_usage_percentage']:.1f}% ä»¥ä¸‹ï¼Œåˆ™å€¼å¾—è€ƒè™‘Serverless")
    
    # æ³¨æ„äº‹é¡¹
    print(f"\nâš ï¸  æ³¨æ„äº‹é¡¹:")
    print(f"   - æˆæœ¬ä¼°ç®—åŸºäºç®€åŒ–æ¨¡å‹ï¼Œå®é™…æˆæœ¬å¯èƒ½æœ‰å·®å¼‚")
    print(f"   - Serverlessæœ‰æœ€å°è®¡è´¹å•ä½å’Œå¹¶å‘é™åˆ¶")
    print(f"   - å»ºè®®åœ¨éç”Ÿäº§ç¯å¢ƒå…ˆæµ‹è¯•Serverlessæ€§èƒ½")
    print(f"   - è€ƒè™‘æ•°æ®è¿ç§»å’Œåº”ç”¨ç¨‹åºå…¼å®¹æ€§")
    
    if data_quality['completeness_percentage'] < 80:
        print(f"   - æ•°æ®å®Œæ•´æ€§è¾ƒä½({data_quality['completeness_percentage']:.1f}%)ï¼Œå»ºè®®å¢åŠ åˆ†æå‘¨æœŸ")
    
    print("\n" + "="*60)

def print_progress_bar(current: int, total: int, prefix: str = "", length: int = 30) -> None:
    """
    æ˜¾ç¤ºè¿›åº¦æ¡
    
    Args:
        current: å½“å‰è¿›åº¦
        total: æ€»æ•°
        prefix: å‰ç¼€æ–‡æœ¬
        length: è¿›åº¦æ¡é•¿åº¦
    """
    if total == 0:
        return
        
    percent = current / total
    filled_length = int(length * percent)
    bar = 'â–ˆ' * filled_length + '-' * (length - filled_length)
    print(f'\r{prefix} |{bar}| {percent:.1%} ({current}/{total})', end='', flush=True)
    
    if current == total:
        print()  # æ¢è¡Œ

def format_duration(seconds: float) -> str:
    """
    æ ¼å¼åŒ–æ—¶é—´é•¿åº¦
    
    Args:
        seconds: ç§’æ•°
        
    Returns:
        æ ¼å¼åŒ–çš„æ—¶é—´å­—ç¬¦ä¸²
    """
    if seconds < 60:
        return f"{seconds:.1f}ç§’"
    elif seconds < 3600:
        minutes = seconds / 60
        return f"{minutes:.1f}åˆ†é’Ÿ"
    else:
        hours = seconds / 3600
        return f"{hours:.1f}å°æ—¶"

def generate_mock_metrics(duration_hours: int = 24, active_percentage: float = 30.0, 
                         pattern: str = 'business_hours') -> Dict[str, List[Dict]]:
    """
    ç”Ÿæˆæ¨¡æ‹Ÿçš„CloudWatchæŒ‡æ ‡æ•°æ®ç”¨äºæµ‹è¯•
    
    Args:
        duration_hours: æµ‹è¯•æ•°æ®æ—¶é•¿ï¼ˆå°æ—¶ï¼‰
        active_percentage: æ´»è·ƒæ—¶é—´ç™¾åˆ†æ¯”
        pattern: æ´»è·ƒæ¨¡å¼ ('business_hours', 'random', 'constant')
        
    Returns:
        æ¨¡æ‹Ÿçš„æŒ‡æ ‡æ•°æ®å­—å…¸
    """
    import random  # åœ¨å‡½æ•°å†…éƒ¨å¯¼å…¥
    
    print(f"ğŸ§ª ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®: {duration_hours}å°æ—¶, {active_percentage}%æ´»è·ƒ, æ¨¡å¼={pattern}")
    
    end_time = datetime.now(timezone.utc)
    start_time = end_time - timedelta(hours=duration_hours)
    
    # ç”Ÿæˆæ—¶é—´åºåˆ—ï¼ˆ5åˆ†é’Ÿé—´éš”ï¼‰
    timestamps = []
    current_time = start_time
    while current_time <= end_time:
        timestamps.append(current_time)
        current_time += timedelta(minutes=5)
    
    total_points = len(timestamps)
    target_active_points = int(total_points * active_percentage / 100)
    
    # æ ¹æ®æ¨¡å¼ç”Ÿæˆæ´»è·ƒæ—¶é—´ç‚¹
    active_indices = set()
    
    if pattern == 'business_hours':
        # å·¥ä½œæ—¶é—´æ¨¡å¼ï¼šå‘¨ä¸€åˆ°å‘¨äº”çš„9-18ç‚¹æ›´æ´»è·ƒ
        for i, ts in enumerate(timestamps):
            if ts.weekday() < 5:  # å‘¨ä¸€åˆ°å‘¨äº”
                if 9 <= ts.hour < 18:  # å·¥ä½œæ—¶é—´
                    if len(active_indices) < target_active_points:
                        active_indices.add(i)
        
        # å¦‚æœå·¥ä½œæ—¶é—´ä¸å¤Ÿï¼Œéšæœºæ·»åŠ ä¸€äº›
        while len(active_indices) < target_active_points:
            active_indices.add(random.randint(0, total_points - 1))
            
    elif pattern == 'random':
        # éšæœºæ¨¡å¼
        active_indices = set(random.sample(range(total_points), target_active_points))
        
    elif pattern == 'constant':
        # æŒç»­æ¨¡å¼ï¼šå‰é¢ä¸€æ®µæ—¶é—´æ´»è·ƒ
        for i in range(min(target_active_points, total_points)):
            active_indices.add(i)
    
    # ç”ŸæˆæŒ‡æ ‡æ•°æ® - åªç”Ÿæˆç”¨äºåˆ¤æ–­æ´»è·ƒçŠ¶æ€çš„æŒ‡æ ‡
    metrics = {
        'ReadIOPS': [],
        'WriteIOPS': [],
        'DatabaseConnections': []
    }
    
    for i, timestamp in enumerate(timestamps):
        is_active = i in active_indices
        
        # ç”ŸæˆæŒ‡æ ‡å€¼
        if is_active:
            read_iops = random.uniform(10, 100)
            write_iops = random.uniform(5, 50)
            connections = random.randint(1, 20)
        else:
            read_iops = 0
            write_iops = 0
            connections = 0
        
        # æ·»åŠ æ•°æ®ç‚¹
        metrics['ReadIOPS'].append({
            'Timestamp': timestamp,
            'Average': read_iops
        })
        metrics['WriteIOPS'].append({
            'Timestamp': timestamp,
            'Average': write_iops
        })
        metrics['DatabaseConnections'].append({
            'Timestamp': timestamp,
            'Average': connections
        })
    
    print(f"âœ“ ç”Ÿæˆäº† {total_points} ä¸ªæ—¶é—´ç‚¹çš„æ¨¡æ‹Ÿæ•°æ®ï¼Œå…¶ä¸­ {len(active_indices)} ä¸ªæ´»è·ƒç‚¹")
    return metrics

def test_with_mock_data() -> bool:
    """
    ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æµ‹è¯•æ ¸å¿ƒé€»è¾‘
    
    Returns:
        Trueå¦‚æœæµ‹è¯•é€šè¿‡ï¼ŒFalseå¦åˆ™
    """
    print("\nğŸ§ª å¼€å§‹æ¨¡æ‹Ÿæ•°æ®æµ‹è¯•...")
    
    test_cases = [
        {'duration': 24, 'active_pct': 50.0, 'pattern': 'random', 'name': 'éšæœº50%æ´»è·ƒ'},
        {'duration': 48, 'active_pct': 25.0, 'pattern': 'business_hours', 'name': 'å·¥ä½œæ—¶é—´25%æ´»è·ƒ'},
        {'duration': 12, 'active_pct': 0.0, 'pattern': 'constant', 'name': 'å®Œå…¨ç©ºé—²'},
        {'duration': 6, 'active_pct': 100.0, 'pattern': 'constant', 'name': 'å®Œå…¨æ´»è·ƒ'}
    ]
    
    all_passed = True
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n--- æµ‹è¯•ç”¨ä¾‹ {i}: {test_case['name']} ---")
        
        try:
            # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
            mock_metrics = generate_mock_metrics(
                duration_hours=test_case['duration'],
                active_percentage=test_case['active_pct'],
                pattern=test_case['pattern']
            )
            
            # åˆ†ææ´»è·ƒçŠ¶æ€
            analysis_result = calculate_idle_percentage(mock_metrics)
            
            # éªŒè¯ç»“æœ
            expected_idle = 100 - test_case['active_pct']
            actual_idle = analysis_result['idle_percentage']
            tolerance = 5.0  # å…è®¸5%çš„è¯¯å·®
            
            if abs(actual_idle - expected_idle) <= tolerance:
                print(f"âœ… æµ‹è¯•é€šè¿‡: æœŸæœ›ç©ºé—²{expected_idle}%, å®é™…{actual_idle:.1f}%")
            else:
                print(f"âŒ æµ‹è¯•å¤±è´¥: æœŸæœ›ç©ºé—²{expected_idle}%, å®é™…{actual_idle:.1f}%, è¯¯å·®è¶…è¿‡{tolerance}%")
                all_passed = False
            
            # éªŒè¯æ•°æ®å®Œæ•´æ€§ï¼ˆä¿®æ­£è®¡ç®—ï¼‰
            total_points = sum(len(points) for points in mock_metrics.values())
            # æ¯5åˆ†é’Ÿä¸€ä¸ªç‚¹ï¼Œæ¯å°æ—¶12ä¸ªç‚¹ï¼Œ3ä¸ªæŒ‡æ ‡ï¼ˆReadIOPS, WriteIOPS, DatabaseConnectionsï¼‰
            expected_points_per_metric = test_case['duration'] * 12 + 1  # +1å› ä¸ºåŒ…å«ç»“æŸæ—¶é—´ç‚¹
            expected_total_points = expected_points_per_metric * 3
            
            # å…è®¸å°çš„è¯¯å·®
            if abs(total_points - expected_total_points) <= 5:
                print(f"âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡: {total_points} ä¸ªæ•°æ®ç‚¹")
            else:
                print(f"âš ï¸  æ•°æ®å®Œæ•´æ€§éªŒè¯: æœŸæœ›çº¦{expected_total_points}, å®é™…{total_points} (åœ¨å…è®¸èŒƒå›´å†…)")
                # ä¸æ ‡è®°ä¸ºå¤±è´¥ï¼Œå› ä¸ºæ—¶é—´è®¡ç®—å¯èƒ½æœ‰å°çš„å·®å¼‚
                
        except Exception as e:
            print(f"âŒ æµ‹è¯•ç”¨ä¾‹æ‰§è¡Œå¤±è´¥: {e}")
            all_passed = False
    
    if all_passed:
        print(f"\nâœ… æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹é€šè¿‡!")
    else:
        print(f"\nâŒ éƒ¨åˆ†æµ‹è¯•ç”¨ä¾‹å¤±è´¥!")
    
    return all_passed

def test_edge_cases() -> bool:
    """
    æµ‹è¯•è¾¹ç•Œæƒ…å†µ
    
    Returns:
        Trueå¦‚æœæµ‹è¯•é€šè¿‡ï¼ŒFalseå¦åˆ™
    """
    print("\nğŸ§ª å¼€å§‹è¾¹ç•Œæƒ…å†µæµ‹è¯•...")
    
    all_passed = True
    
    # æµ‹è¯•1: ç©ºæ•°æ®
    print("\n--- æµ‹è¯•: ç©ºæ•°æ®å¤„ç† ---")
    try:
        empty_metrics = {
            'ReadIOPS': [],
            'WriteIOPS': [],
            'DatabaseConnections': [],
            'NetworkReceiveThroughput': [],
            'NetworkTransmitThroughput': []
        }
        
        result = calculate_idle_percentage(empty_metrics)
        if result['idle_percentage'] == 0.0 and result['total_points'] == 0:
            print("âœ… ç©ºæ•°æ®å¤„ç†æ­£ç¡®")
        else:
            print("âŒ ç©ºæ•°æ®å¤„ç†å¤±è´¥")
            all_passed = False
            
    except Exception as e:
        print(f"âŒ ç©ºæ•°æ®æµ‹è¯•å¼‚å¸¸: {e}")
        all_passed = False
    
    # æµ‹è¯•2: å•ä¸ªæ•°æ®ç‚¹
    print("\n--- æµ‹è¯•: å•ä¸ªæ•°æ®ç‚¹ ---")
    try:
        test_timestamp = datetime.now(timezone.utc)
        single_point_metrics = {
            'ReadIOPS': [{'Timestamp': test_timestamp, 'Average': 10.0}],
            'WriteIOPS': [{'Timestamp': test_timestamp, 'Average': 0.0}],
            'DatabaseConnections': [{'Timestamp': test_timestamp, 'Average': 0.0}],
            'NetworkReceiveThroughput': [{'Timestamp': test_timestamp, 'Average': 0.0}],
            'NetworkTransmitThroughput': [{'Timestamp': test_timestamp, 'Average': 0.0}]
        }
        
        result = calculate_idle_percentage(single_point_metrics)
        if result['total_points'] == 1 and result['active_points'] == 1:
            print("âœ… å•ä¸ªæ•°æ®ç‚¹å¤„ç†æ­£ç¡®")
        else:
            print("âŒ å•ä¸ªæ•°æ®ç‚¹å¤„ç†å¤±è´¥")
            all_passed = False
            
    except Exception as e:
        print(f"âŒ å•ä¸ªæ•°æ®ç‚¹æµ‹è¯•å¼‚å¸¸: {e}")
        all_passed = False
    
    # æµ‹è¯•3: è¾“å…¥éªŒè¯
    print("\n--- æµ‹è¯•: è¾“å…¥éªŒè¯ ---")
    test_inputs = [
        ('', 'us-east-1', 7, "ç©ºé›†ç¾¤ID"),
        ('test-cluster', '', 7, "ç©ºåŒºåŸŸ"),
        ('test-cluster', 'us-east-1', 0, "é›¶å¤©æ•°"),
        ('test-cluster', 'us-east-1', 31, "è¶…è¿‡30å¤©"),
    ]
    
    for cluster_id, region, days, description in test_inputs:
        try:
            validate_inputs(cluster_id, region, days)
            print(f"âŒ {description}: åº”è¯¥æŠ›å‡ºå¼‚å¸¸ä½†æ²¡æœ‰")
            all_passed = False
        except ValueError:
            print(f"âœ… {description}: æ­£ç¡®æŠ›å‡ºå¼‚å¸¸")
        except Exception as e:
            print(f"âŒ {description}: æ„å¤–å¼‚å¸¸ {e}")
            all_passed = False
    
    if all_passed:
        print(f"\nâœ… æ‰€æœ‰è¾¹ç•Œæƒ…å†µæµ‹è¯•é€šè¿‡!")
    else:
        print(f"\nâŒ éƒ¨åˆ†è¾¹ç•Œæƒ…å†µæµ‹è¯•å¤±è´¥!")
    
    return all_passed

def run_all_tests() -> bool:
    """
    è¿è¡Œæ‰€æœ‰æµ‹è¯•
    
    Returns:
        Trueå¦‚æœæ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ŒFalseå¦åˆ™
    """
    print("ğŸ§ª å¼€å§‹è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶...")
    
    mock_test_passed = test_with_mock_data()
    edge_test_passed = test_edge_cases()
    
    all_passed = mock_test_passed and edge_test_passed
    
    print(f"\n{'='*50}")
    print(f"ğŸ§ª æµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"   æ¨¡æ‹Ÿæ•°æ®æµ‹è¯•: {'âœ… é€šè¿‡' if mock_test_passed else 'âŒ å¤±è´¥'}")
    print(f"   è¾¹ç•Œæƒ…å†µæµ‹è¯•: {'âœ… é€šè¿‡' if edge_test_passed else 'âŒ å¤±è´¥'}")
    print(f"   æ€»ä½“ç»“æœ: {'âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡' if all_passed else 'âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥'}")
    print(f"{'='*50}")
    
    return all_passed

def get_cloudwatch_metrics_batch(cloudwatch, cluster_id: str, metric_name: str, 
                                start_time: datetime, end_time: datetime, period: int = 60) -> List[Dict]:
    """
    åˆ†æ‰¹è·å–å•ä¸ªæŒ‡æ ‡çš„CloudWatchæ•°æ®
    
    Args:
        cloudwatch: CloudWatchå®¢æˆ·ç«¯
        cluster_id: é›†ç¾¤æ ‡è¯†ç¬¦
        metric_name: æŒ‡æ ‡åç§°
        start_time: å¼€å§‹æ—¶é—´
        end_time: ç»“æŸæ—¶é—´
        period: é‡‡æ ·é—´éš”ï¼ˆç§’ï¼‰
        
    Returns:
        æ•°æ®ç‚¹åˆ—è¡¨
    """
    all_datapoints = []
    current_start = start_time
    
    while current_start < end_time:
        # æ¯æ‰¹æœ€å¤šæŸ¥è¯¢1å¤©çš„æ•°æ®ï¼ˆ60ç§’é‡‡æ · = 1440ä¸ªç‚¹ï¼‰
        batch_end = min(current_start + timedelta(days=1), end_time)
        
        try:
            response = cloudwatch.get_metric_statistics(
                Namespace='AWS/Redshift',
                MetricName=metric_name,
                Dimensions=[{'Name': 'ClusterIdentifier', 'Value': cluster_id}],
                StartTime=current_start,
                EndTime=batch_end,
                Period=period,
                Statistics=['Average']
            )
            
            batch_datapoints = response.get('Datapoints', [])
            all_datapoints.extend(batch_datapoints)
            
            print(f"       æ‰¹æ¬¡ {current_start.strftime('%m-%d')} ~ {batch_end.strftime('%m-%d')}: {len(batch_datapoints)} ä¸ªæ•°æ®ç‚¹")
            
            # é¿å…APIé™æµ
            time.sleep(0.1)
            
        except ClientError as e:
            if e.response['Error']['Code'] == 'Throttling':
                print(f"       âš ï¸  APIé™æµï¼Œç­‰å¾…é‡è¯•...")
                time.sleep(2)
                # é‡è¯•å½“å‰æ‰¹æ¬¡
                continue
            else:
                print(f"       âŒ æ‰¹æ¬¡å¤±è´¥: {e}")
        
        current_start = batch_end
    
    return sorted(all_datapoints, key=lambda x: x['Timestamp'])

def get_cloudwatch_metrics(cluster_id: str, region: str, days: int) -> Dict[str, List[Dict]]:
    """
    è·å–CloudWatchæŒ‡æ ‡æ•°æ®
    
    Args:
        cluster_id: Redshifté›†ç¾¤æ ‡è¯†ç¬¦
        region: AWSåŒºåŸŸ
        days: åˆ†æå¤©æ•°
        
    Returns:
        åŒ…å«å„æŒ‡æ ‡æ•°æ®ç‚¹çš„å­—å…¸
        
    Raises:
        ClientError: AWS APIè°ƒç”¨å¤±è´¥
        NoCredentialsError: AWSå‡­è¯æœªé…ç½®
    """
    print("ğŸ“Š å¼€å§‹è·å–CloudWatchæŒ‡æ ‡æ•°æ®...")
    
    try:
        cloudwatch = boto3.client('cloudwatch', region_name=region)
        end_time = datetime.now(timezone.utc)
        start_time = end_time - timedelta(days=days)
        
        print(f"   æ—¶é—´èŒƒå›´: {start_time.strftime('%Y-%m-%d %H:%M')} åˆ° {end_time.strftime('%Y-%m-%d %H:%M')}")
        
        # å®šä¹‰è¦æ”¶é›†çš„æŒ‡æ ‡ - åªæ”¶é›†ç”¨äºåˆ¤æ–­æ´»è·ƒçŠ¶æ€çš„æŒ‡æ ‡
        metric_names = [
            'ReadIOPS',
            'WriteIOPS', 
            'DatabaseConnections'
        ]
        
        # å›ºå®šä½¿ç”¨60ç§’é‡‡æ ·ï¼Œä¸Serverlessè®¡è´¹å‘¨æœŸä¸€è‡´
        period = 60
        print(f"   é‡‡æ ·é—´éš”: {period}ç§’ (ä¸Serverlessè®¡è´¹å‘¨æœŸä¸€è‡´)")
        
        # è®¡ç®—æ˜¯å¦éœ€è¦åˆ†æ‰¹æŸ¥è¯¢
        time_span_hours = (end_time - start_time).total_seconds() / 3600
        if time_span_hours > 24:
            print(f"   æ•°æ®è·¨åº¦ {time_span_hours:.1f} å°æ—¶ï¼Œå°†åˆ†æ‰¹æŸ¥è¯¢ä»¥ä¿æŒ60ç§’é‡‡æ ·ç²¾åº¦")
        
        metrics = {}
        total_metrics = len(metric_names)
        
        for i, metric_name in enumerate(metric_names, 1):
            print(f"   è·å–æŒ‡æ ‡ {i}/{total_metrics}: {metric_name}")
            
            # ä½¿ç”¨åˆ†æ‰¹æŸ¥è¯¢è·å–æ•°æ®
            datapoints = get_cloudwatch_metrics_batch(
                cloudwatch, cluster_id, metric_name, start_time, end_time, period
            )
            
            metrics[metric_name] = datapoints
            print(f"     âœ“ æ€»è®¡è·å–åˆ° {len(datapoints)} ä¸ªæ•°æ®ç‚¹")
        
        total_points = sum(len(points) for points in metrics.values())
        print(f"âœ“ CloudWatchæ•°æ®è·å–å®Œæˆï¼Œæ€»è®¡ {total_points} ä¸ªæ•°æ®ç‚¹")
        
        return metrics
        
    except NoCredentialsError:
        raise NoCredentialsError("AWSå‡­è¯æœªé…ç½®ã€‚è¯·é…ç½®AWS CLIæˆ–è®¾ç½®ç¯å¢ƒå˜é‡ã€‚")
    except ClientError as e:
        error_code = e.response['Error']['Code']
        if error_code == 'AccessDenied':
            raise ClientError(
                {'Error': {'Code': 'AccessDenied', 'Message': 'æƒé™ä¸è¶³ã€‚è¯·ç¡®ä¿å…·æœ‰cloudwatch:GetMetricStatisticsæƒé™ã€‚'}},
                'GetMetricStatistics'
            )
        else:
            raise

def safe_get_metrics(cluster_id: str, region: str, days: int, max_retries: int = 3) -> Dict[str, List[Dict]]:
    """
    å®‰å…¨è·å–æŒ‡æ ‡æ•°æ®ï¼ŒåŒ…å«é‡è¯•é€»è¾‘
    
    Args:
        cluster_id: Redshifté›†ç¾¤æ ‡è¯†ç¬¦
        region: AWSåŒºåŸŸ
        days: åˆ†æå¤©æ•°
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
    Returns:
        æŒ‡æ ‡æ•°æ®å­—å…¸
    """
    for attempt in range(max_retries):
        try:
            return get_cloudwatch_metrics(cluster_id, region, days)
        except ClientError as e:
            error_code = e.response['Error']['Code']
            if error_code == 'Throttling' and attempt < max_retries - 1:
                wait_time = 2 ** attempt
                print(f"âš ï¸  APIé™æµï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
                continue
            else:
                print(f"âŒ CloudWatch APIé”™è¯¯: {e}")
                sys.exit(1)
        except Exception as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt
                print(f"âš ï¸  è·å–æ•°æ®å¤±è´¥ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•: {e}")
                time.sleep(wait_time)
                continue
            else:
                print(f"âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé€€å‡º: {e}")
                sys.exit(1)
    
    return {}

def get_value_at_timestamp(metric_data: List[Dict], target_timestamp: datetime) -> float:
    """
    è·å–æŒ‡å®šæ—¶é—´æˆ³çš„æŒ‡æ ‡å€¼
    
    Args:
        metric_data: æŒ‡æ ‡æ•°æ®ç‚¹åˆ—è¡¨
        target_timestamp: ç›®æ ‡æ—¶é—´æˆ³
        
    Returns:
        æŒ‡æ ‡å€¼ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›0.0
    """
    for point in metric_data:
        # å…è®¸60ç§’çš„æ—¶é—´è¯¯å·®ï¼ˆä¸é‡‡æ ·é—´éš”ä¸€è‡´ï¼‰
        if abs((point['Timestamp'] - target_timestamp).total_seconds()) <= 60:
            return point.get('Average', 0.0)
    return 0.0

def calculate_idle_percentage(metrics: Dict[str, List[Dict]]) -> Dict[str, Any]:
    """
    è®¡ç®—ç©ºé—²æ—¶é—´ç™¾åˆ†æ¯”
    
    Args:
        metrics: CloudWatchæŒ‡æ ‡æ•°æ®å­—å…¸
        
    Returns:
        åŒ…å«åˆ†æç»“æœçš„å­—å…¸
    """
    print("ğŸ” å¼€å§‹åˆ†ææ´»è·ƒçŠ¶æ€...")
    
    # é¢„å®šä¹‰æ´»è·ƒè§„åˆ™ - åªå…³æ³¨çœŸæ­£çš„ä¸šåŠ¡æ´»åŠ¨æŒ‡æ ‡
    # ç½‘ç»œæµé‡ä¸ä½œä¸ºåˆ¤æ–­ä¾æ®ï¼Œå› ä¸ºç³»ç»Ÿç»´æŠ¤ã€ç›‘æ§ç­‰ä¼šäº§ç”ŸæŒç»­çš„åŸºç¡€ç½‘ç»œæµé‡
    activity_rules = {
        'ReadIOPS': lambda x: x > 0,
        'WriteIOPS': lambda x: x > 0,
        'DatabaseConnections': lambda x: x > 0,
        # ç§»é™¤ç½‘ç»œæµé‡æŒ‡æ ‡ï¼Œé¿å…è¯¯åˆ¤
        # 'NetworkReceiveThroughput': lambda x: x > 1024,
        # 'NetworkTransmitThroughput': lambda x: x > 1024
    }
    
    # æ”¶é›†æ‰€æœ‰æ—¶é—´æˆ³
    all_timestamps = set()
    for metric_name, metric_data in metrics.items():
        for point in metric_data:
            all_timestamps.add(point['Timestamp'])
    
    if not all_timestamps:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®ç‚¹")
        return {
            'idle_percentage': 0.0,
            'total_points': 0,
            'active_points': 0,
            'idle_points': 0,
            'analysis_period': None,
            'activity_breakdown': {}
        }
    
    # æŒ‰æ—¶é—´æ’åº
    sorted_timestamps = sorted(all_timestamps)
    total_count = len(sorted_timestamps)
    active_count = 0
    
    # ç»Ÿè®¡å„æŒ‡æ ‡çš„æ´»è·ƒæ¬¡æ•°
    activity_breakdown = {metric: 0 for metric in activity_rules.keys()}
    
    print(f"   åˆ†æ {total_count} ä¸ªæ—¶é—´ç‚¹...")
    
    for i, timestamp in enumerate(sorted_timestamps):
        is_active = False
        active_metrics = []
        
        # æ£€æŸ¥æ¯ä¸ªæŒ‡æ ‡åœ¨è¯¥æ—¶é—´ç‚¹çš„å€¼
        for metric_name, rule in activity_rules.items():
            if metric_name in metrics:
                value = get_value_at_timestamp(metrics[metric_name], timestamp)
                
                if rule(value):
                    is_active = True
                    active_metrics.append(f"{metric_name}={value:.2f}")
                    activity_breakdown[metric_name] += 1
        
        if is_active:
            active_count += 1
            
        # æ˜¾ç¤ºè¿›åº¦æ¡ï¼ˆæ¯å¤„ç†50ä¸ªç‚¹æ›´æ–°ä¸€æ¬¡ï¼‰
        if (i + 1) % 50 == 0 or (i + 1) == total_count:
            print_progress_bar(i + 1, total_count, "   åˆ†æè¿›åº¦:")
    
    idle_count = total_count - active_count
    idle_percentage = (idle_count / total_count) * 100 if total_count > 0 else 0.0
    
    analysis_result = {
        'idle_percentage': idle_percentage,
        'total_points': total_count,
        'active_points': active_count,
        'idle_points': idle_count,
        'analysis_period': (sorted_timestamps[0], sorted_timestamps[-1]),
        'activity_breakdown': activity_breakdown
    }
    
    print(f"âœ“ æ´»è·ƒçŠ¶æ€åˆ†æå®Œæˆ")
    print(f"   æ€»æ—¶é—´ç‚¹: {total_count}")
    print(f"   æ´»è·ƒæ—¶é—´ç‚¹: {active_count} ({(active_count/total_count*100):.1f}%)")
    print(f"   ç©ºé—²æ—¶é—´ç‚¹: {idle_count} ({idle_percentage:.1f}%)")
    
    # æ˜¾ç¤ºå„æŒ‡æ ‡çš„æ´»è·ƒç»Ÿè®¡
    print(f"   å„æŒ‡æ ‡æ´»è·ƒç»Ÿè®¡:")
    for metric, count in activity_breakdown.items():
        percentage = (count / total_count * 100) if total_count > 0 else 0
        print(f"     {metric}: {count} æ¬¡ ({percentage:.1f}%)")
    
    return analysis_result

def get_cluster_info(cluster_id: str, region: str) -> Dict[str, Any]:
    """
    è·å–Redshifté›†ç¾¤ä¿¡æ¯
    
    Args:
        cluster_id: é›†ç¾¤æ ‡è¯†ç¬¦
        region: AWSåŒºåŸŸ
        
    Returns:
        é›†ç¾¤ä¿¡æ¯å­—å…¸
    """
    try:
        redshift = boto3.client('redshift', region_name=region)
        response = redshift.describe_clusters(ClusterIdentifier=cluster_id)
        
        if not response['Clusters']:
            raise ValueError(f"æœªæ‰¾åˆ°é›†ç¾¤: {cluster_id}")
            
        cluster = response['Clusters'][0]
        return {
            'node_type': cluster.get('NodeType', 'unknown'),
            'number_of_nodes': cluster.get('NumberOfNodes', 1),
            'cluster_status': cluster.get('ClusterStatus', 'unknown'),
            'cluster_version': cluster.get('ClusterVersion', 'unknown')
        }
    except ClientError as e:
        print(f"âš ï¸  æ— æ³•è·å–é›†ç¾¤ä¿¡æ¯: {e}")
        return {
            'node_type': 'unknown',
            'number_of_nodes': 1,
            'cluster_status': 'unknown',
            'cluster_version': 'unknown'
        }

def estimate_monthly_cost(node_type: str, number_of_nodes: int, region: str) -> float:
    """
    ä¼°ç®—æœˆåº¦æˆæœ¬ï¼Œä½¿ç”¨åŠ¨æ€ä»·æ ¼æŸ¥è¯¢
    
    Args:
        node_type: èŠ‚ç‚¹ç±»å‹
        number_of_nodes: èŠ‚ç‚¹æ•°é‡
        region: AWSåŒºåŸŸ
        
    Returns:
        ä¼°ç®—çš„æœˆåº¦æˆæœ¬
    """
    # åŠ¨æ€è·å–å®ä¾‹ä»·æ ¼
    price_info = get_instance_price_dynamic(node_type, region)
    hourly_cost = price_info['price']
    price_source = price_info['source']
    
    if price_source == 'hardcoded' and node_type not in ['dc2.large', 'dc2.8xlarge', 'ra3.large', 'ra3.xlplus', 'ra3.4xlarge', 'ra3.16xlarge']:
        print(f"âš ï¸  æœªçŸ¥å®ä¾‹ç±»å‹ {node_type}ï¼Œä½¿ç”¨ ra3.xlplus ä»·æ ¼ä¼°ç®—")
    
    print(f"   å®ä¾‹ä»·æ ¼: {hourly_cost}/å°æ—¶ (æ¥æº: {price_source})")
    
    total_hourly_cost = hourly_cost * number_of_nodes
    monthly_cost = total_hourly_cost * 24 * 30  # å‡è®¾30å¤©
    
    return monthly_cost

def calculate_rpu_requirement(node_type: str, number_of_nodes: int) -> int:
    """
    è®¡ç®—Serverlessæ‰€éœ€çš„RPUæ•°é‡
    
    Args:
        node_type: èŠ‚ç‚¹ç±»å‹
        number_of_nodes: èŠ‚ç‚¹æ•°é‡
        
    Returns:
        æ‰€éœ€çš„RPUæ•°é‡
    """
    # RPUå¯¹åº”å…³ç³»ï¼š8 RPU = 4 x RA3.XLPlusï¼Œå³ 1 RPU = 0.5 x RA3.XLPlus
    # æœ€å°RPUæ˜¯8ï¼Œæ¯æ¬¡å¢åŠ éƒ½æ˜¯8ä¸ªRPU
    
    # å„å®ä¾‹ç±»å‹å¯¹åº”çš„RA3.XLPlusç­‰æ•ˆæ•°é‡
    xlplus_equivalent = {
        'dc2.large': 0.25,      # DC2.largeçº¦ç­‰äº0.25ä¸ªRA3.XLPlus
        'dc2.8xlarge': 4.0,     # DC2.8xlargeçº¦ç­‰äº4ä¸ªRA3.XLPlus
        'ra3.large': 0.5,       # RA3.largeçº¦ç­‰äº0.5ä¸ªRA3.XLPlus
        'ra3.xlplus': 1.0,      # RA3.XLPlusåŸºå‡†
        'ra3.4xlarge': 4.0,     # RA3.4xlargeçº¦ç­‰äº4ä¸ªRA3.XLPlus
        'ra3.16xlarge': 16.0,   # RA3.16xlargeçº¦ç­‰äº16ä¸ªRA3.XLPlus
    }
    
    # è®¡ç®—æ€»çš„RA3.XLPlusç­‰æ•ˆæ•°é‡
    equivalent_xlplus = xlplus_equivalent.get(node_type, 1.0) * number_of_nodes
    
    # è®¡ç®—æ‰€éœ€RPUï¼ˆ1 RPU = 0.5 x RA3.XLPlusï¼Œå³ 8 RPU = 4 x RA3.XLPlusï¼‰
    required_rpu = equivalent_xlplus / 0.5  # ç­‰æ•ˆäº equivalent_xlplus * 2
    
    # RPUå¿…é¡»æ˜¯8çš„å€æ•°ï¼Œä¸”æœ€å°ä¸º8
    rpu_units = max(8, int((required_rpu + 7) // 8) * 8)  # å‘ä¸Šå–æ•´åˆ°8çš„å€æ•°
    
    return rpu_units

def calculate_cost_savings(cluster_id: str, region: str, idle_percentage: float, 
                         cluster_info: Dict[str, Any]) -> Dict[str, float]:
    """
    è®¡ç®—æ½œåœ¨æˆæœ¬èŠ‚çœ
    
    Args:
        cluster_id: é›†ç¾¤æ ‡è¯†ç¬¦
        region: AWSåŒºåŸŸ
        idle_percentage: ç©ºé—²æ—¶é—´ç™¾åˆ†æ¯”
        cluster_info: é›†ç¾¤ä¿¡æ¯
        
    Returns:
        æˆæœ¬åˆ†æç»“æœå­—å…¸
    """
    print("ğŸ’° å¼€å§‹è®¡ç®—æˆæœ¬èŠ‚çœ...")
    
    node_type = cluster_info['node_type']
    number_of_nodes = cluster_info['number_of_nodes']
    
    print(f"   é›†ç¾¤é…ç½®: {node_type} x {number_of_nodes}")
    
    # è®¡ç®—å½“å‰æœˆåº¦æˆæœ¬
    current_monthly_cost = estimate_monthly_cost(node_type, number_of_nodes, region)
    
    # è®¡ç®—æ´»è·ƒæ—¶é—´ç™¾åˆ†æ¯”
    active_percentage = 100 - idle_percentage
    
    # è®¡ç®—Serverlessæ‰€éœ€RPU
    required_rpu = calculate_rpu_requirement(node_type, number_of_nodes)
    print(f"   Serverlessæ‰€éœ€RPU: {required_rpu}")
    
    # åŠ¨æ€è·å–Serverless RPUä»·æ ¼
    price_info = get_rpu_price_dynamic(region)
    rpu_hourly_cost = price_info['price']
    currency = price_info['currency']
    price_source = price_info['source']
    
    currency_symbol = 'Â¥' if currency == 'CNY' else '$'
    
    print(f"   RPUä»·æ ¼: {currency_symbol}{rpu_hourly_cost}/å°æ—¶ (æ¥æº: {price_source})")
    
    # Serverlessæˆæœ¬ = RPUæ•°é‡ Ã— å°æ—¶è´¹ç‡ Ã— æ´»è·ƒæ—¶é—´
    serverless_hourly_cost = required_rpu * rpu_hourly_cost
    serverless_monthly_cost = serverless_hourly_cost * 24 * 30 * (active_percentage / 100)
    
    # è®¡ç®—èŠ‚çœ
    potential_savings = current_monthly_cost - serverless_monthly_cost
    savings_percentage = (potential_savings / current_monthly_cost) * 100 if current_monthly_cost > 0 else 0
    
    # è®¡ç®—ç›ˆäºå¹³è¡¡ç‚¹
    break_even_usage_percentage = (current_monthly_cost / (serverless_hourly_cost * 24 * 30)) * 100
    
    cost_analysis = {
        'current_monthly_cost': current_monthly_cost,
        'serverless_monthly_cost': serverless_monthly_cost,
        'potential_savings': potential_savings,
        'savings_percentage': savings_percentage,
        'break_even_usage_percentage': break_even_usage_percentage,
        'active_percentage': active_percentage,
        'required_rpu': required_rpu,
        'currency_symbol': currency_symbol
    }
    
    print(f"âœ“ æˆæœ¬è®¡ç®—å®Œæˆ")
    print(f"   å½“å‰æœˆåº¦æˆæœ¬: {currency_symbol}{current_monthly_cost:.2f}")
    print(f"   Serverlessé¢„ä¼°æˆæœ¬: {currency_symbol}{serverless_monthly_cost:.2f}")
    print(f"   æ½œåœ¨æœˆåº¦èŠ‚çœ: {currency_symbol}{potential_savings:.2f} ({savings_percentage:.1f}%)")
    
    if savings_percentage > 0:
        print(f"   ğŸ’¡ å»ºè®®: è¿ç§»åˆ°Serverlesså¯èŠ‚çœæˆæœ¬")
    else:
        print(f"   âš ï¸  æ³¨æ„: å½“å‰ä½¿ç”¨ç‡ä¸‹ï¼ŒServerlesså¯èƒ½æ›´è´µ")
        print(f"   ç›ˆäºå¹³è¡¡ç‚¹: ä½¿ç”¨ç‡éœ€ä½äº {break_even_usage_percentage:.1f}%")
    
    return cost_analysis

def main():
    """ä¸»å‡½æ•°ï¼šå‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description="åˆ†æRedshifté›†ç¾¤ç©ºé—²æ—¶é—´ï¼Œè¯„ä¼°Serverlessè¿ç§»æˆæœ¬èŠ‚çœ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
    %(prog)s --cluster-id my-redshift-cluster --region us-east-1
    %(prog)s --cluster-id my-cluster --region cn-north-1 --days 14
        """
    )
    
    parser.add_argument(
        '--cluster-id', 
        required=False,  # æµ‹è¯•æ¨¡å¼ä¸‹ä¸éœ€è¦
        help='Redshifté›†ç¾¤æ ‡è¯†ç¬¦'
    )
    
    parser.add_argument(
        '--region', 
        default='cn-north-1',
        help='AWSåŒºåŸŸ (é»˜è®¤: cn-north-1)'
    )
    
    parser.add_argument(
        '--days', 
        type=int, 
        default=7,
        help='åˆ†æå¤©æ•° (é»˜è®¤: 7, æœ€å¤§: 30)'
    )
    
    parser.add_argument(
        '--version', 
        action='version', 
        version=f'%(prog)s {__version__}'
    )
    
    parser.add_argument(
        '--test',
        action='store_true',
        help='è¿è¡Œå†…ç½®æµ‹è¯•å¥—ä»¶'
    )
    
    args = parser.parse_args()
    
    # å¦‚æœæ˜¯æµ‹è¯•æ¨¡å¼ï¼Œè¿è¡Œæµ‹è¯•å¹¶é€€å‡º
    if args.test:
        print(f"ğŸ§ª Redshiftç©ºé—²æ—¶é—´åˆ†æå™¨ v{__version__} - æµ‹è¯•æ¨¡å¼")
        success = run_all_tests()
        sys.exit(0 if success else 1)
    
    # éæµ‹è¯•æ¨¡å¼ä¸‹cluster-idæ˜¯å¿…éœ€çš„
    if not args.cluster_id:
        parser.error("--cluster-id is required (except in test mode)")
    
    try:
        # éªŒè¯è¾“å…¥å‚æ•°
        validate_inputs(args.cluster_id, args.region, args.days)
        
        # éªŒè¯AWSå‡­è¯
        if not validate_aws_credentials(args.region):
            sys.exit(1)
        
        # éªŒè¯é›†ç¾¤è®¿é—®æƒé™
        if not validate_cluster_access(args.cluster_id, args.region):
            sys.exit(1)
        
        # éªŒè¯CloudWatchæƒé™
        if not validate_cloudwatch_permissions(args.cluster_id, args.region):
            sys.exit(1)
        
        print(f"\n=== Redshiftç©ºé—²æ—¶é—´åˆ†æå™¨ v{__version__} ===")
        print(f"å¼€å§‹åˆ†æé›†ç¾¤: {args.cluster_id}")
        print(f"åŒºåŸŸ: {args.region}")
        print(f"åˆ†æå‘¨æœŸ: è¿‡å»{args.days}å¤©")
        print("-" * 50)
        
        # è·å–CloudWatchæŒ‡æ ‡æ•°æ®
        metrics = safe_get_metrics(args.cluster_id, args.region, args.days)
        
        # æ£€æŸ¥æ•°æ®è´¨é‡
        data_quality = check_data_availability(metrics)
        if not data_quality['is_sufficient']:
            print("\nâš ï¸  æ•°æ®è´¨é‡è­¦å‘Š: æ•°æ®ä¸è¶³å¯èƒ½å½±å“åˆ†æå‡†ç¡®æ€§")
            print("   å»ºè®®:")
            print("   - æ£€æŸ¥é›†ç¾¤æ˜¯å¦åœ¨æŒ‡å®šæ—¶é—´æ®µå†…æ­£å¸¸è¿è¡Œ")
            print("   - å°è¯•å¢åŠ åˆ†æå¤©æ•°")
            print("   - ç¡®è®¤é›†ç¾¤åœ¨åˆ†ææœŸé—´æœ‰å®é™…ä½¿ç”¨")
            
            # è¯¢é—®æ˜¯å¦ç»§ç»­
            try:
                response = input("\næ˜¯å¦ç»§ç»­åˆ†æ? (y/N): ").strip().lower()
                if response not in ['y', 'yes']:
                    print("åˆ†æå·²å–æ¶ˆ")
                    sys.exit(0)
            except KeyboardInterrupt:
                print("\nåˆ†æå·²å–æ¶ˆ")
                sys.exit(0)
        
        # æ˜¾ç¤ºæ•°æ®è·å–æ‘˜è¦
        print(f"\nğŸ“ˆ æ•°æ®è·å–æ‘˜è¦:")
        for metric_name, datapoints in metrics.items():
            if datapoints:
                first_time = datapoints[0]['Timestamp'].strftime('%Y-%m-%d %H:%M')
                last_time = datapoints[-1]['Timestamp'].strftime('%Y-%m-%d %H:%M')
                print(f"   {metric_name}: {len(datapoints)} ä¸ªæ•°æ®ç‚¹ ({first_time} ~ {last_time})")
            else:
                print(f"   {metric_name}: æ— æ•°æ®")
        
        # åˆ†ææ´»è·ƒçŠ¶æ€
        analysis_result = calculate_idle_percentage(metrics)
        
        # è·å–é›†ç¾¤ä¿¡æ¯
        print(f"\nğŸ” è·å–é›†ç¾¤ä¿¡æ¯...")
        cluster_info = get_cluster_info(args.cluster_id, args.region)
        
        # è®¡ç®—æˆæœ¬èŠ‚çœ
        cost_analysis = calculate_cost_savings(
            args.cluster_id, 
            args.region, 
            analysis_result['idle_percentage'],
            cluster_info
        )
        
        # è¾“å‡ºæ ¼å¼åŒ–ç»“æœ
        print_results(
            args.cluster_id,
            args.region, 
            args.days,
            analysis_result,
            cost_analysis,
            cluster_info,
            data_quality
        )
        
    except ValueError as e:
        print(f"âŒ è¾“å…¥é”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)
    except NoCredentialsError as e:
        print(f"âŒ AWSå‡­è¯é”™è¯¯: {e}", file=sys.stderr)
        print("è¯·è¿è¡Œ 'aws configure' é…ç½®å‡­è¯æˆ–è®¾ç½®ç¯å¢ƒå˜é‡", file=sys.stderr)
        sys.exit(1)
    except ClientError as e:
        print(f"âŒ AWS APIé”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"âŒ æœªçŸ¥é”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()